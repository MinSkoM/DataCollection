import React, { useState, useRef, useEffect, useCallback } from 'react';
import { FACEMESH_LANDMARK_INDICES } from './constants';
import type { Scenario, UploadStatus, SensorData, FrameData, FaceMeshResult, Point } from './types';
import { LoadingSpinner, CheckCircleIcon, ExclamationTriangleIcon, RecordIcon, StopIcon } from './components/Icons';

// Helper function to check for iOS
const isIOS = () => /iPhone|iPad|iPod/i.test(navigator.userAgent);

const App: React.FC = () => {
    // Check Library Loading State
    const [libsLoaded, setLibsLoaded] = useState(() => {
        const cv = (window as any).cv;
        const isCvReady = cv && cv.Mat;
        const isFaceMeshReady = 'FaceMesh' in window;
        return !!(isCvReady && isFaceMeshReady);
    });

    const [hasPermission, setHasPermission] = useState(false);
    const [isRecording, setIsRecording] = useState(false);
    
    const [isReviewing, setIsReviewing] = useState(false);
    // Ref for recording state to avoid stale closures
    const isRecordingRef = useRef(false);

    // ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Dropdown
    const [scenario, setScenario] = useState<Scenario>('REAL_Normal');
    
    const [uploadStatus, setUploadStatus] = useState<UploadStatus>('idle');
    const [errorMessage, setErrorMessage] = useState<string | null>(null);

    const videoRef = useRef<HTMLVideoElement>(null);
    const canvasRef = useRef<HTMLCanvasElement>(null);
    const faceMeshRef = useRef<any>(null);
    const animationFrameId = useRef<number | null>(null);

    const sensorDataBuffer = useRef<SensorData[]>([]);
    const recordedData = useRef<FrameData[]>([]);

    // Optical Flow Refs
    const prevGray = useRef<any>(null);
    const backgroundPoints = useRef<any>(null);

    // --- 1. Load Libraries Logic ---
    const loadLibraries = useCallback(() => {
        if (libsLoaded) return;
        const checkMediaPipe = 'FaceMesh' in window;
        const cv = (window as any).cv;
        const isOpenCvReady = cv && cv.Mat; 

        if (isOpenCvReady && checkMediaPipe) {
            setLibsLoaded(true);
        } else {
            if (!cv) { (window as any).cv = {}; }
            if (!(window as any).cv.onRuntimeInitialized) {
                (window as any).cv.onRuntimeInitialized = () => {
                    if ('FaceMesh' in window) setLibsLoaded(true);
                };
            }
        }
    }, [libsLoaded]);

    useEffect(() => {
        loadLibraries();
        const interval = setInterval(() => {
            const cv = (window as any).cv;
            if (cv && cv.Mat && 'FaceMesh' in window) {
                setLibsLoaded(true);
                clearInterval(interval);
            }
        }, 500);
        return () => clearInterval(interval);
    }, [loadLibraries]);

    // --- 2. Initialize FaceMesh ---
    const initializeFaceMesh = useCallback(() => {
        if (!libsLoaded || faceMeshRef.current) return;
        if (!('FaceMesh' in window)) return;

        const faceMesh = new (window as any).FaceMesh({
            locateFile: (file: string) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
        });
        faceMesh.setOptions({
            maxNumFaces: 1,
            refineLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5,
        });
        faceMesh.onResults(onFaceMeshResults);
        faceMeshRef.current = faceMesh;
        console.log("FaceMesh Initialized");
    }, [libsLoaded]);

    // --- 3. Camera Handling ---
    const startCamera = useCallback(async () => {
        if (videoRef.current && videoRef.current.srcObject) return;
        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { facingMode: 'user', width: { ideal: 640 }, height: { ideal: 480 } },
            });
            if (videoRef.current) {
                videoRef.current.srcObject = stream;
            }
            initializeFaceMesh();
        } catch (err) {
            setErrorMessage("Camera access is required.");
        }
    }, [initializeFaceMesh]);

    const handleConnect = async () => {
        if (!isIOS()) {
            setHasPermission(true);
            await startCamera();
            return;
        }
        try {
            const permissionState = await (DeviceMotionEvent as any).requestPermission();
            if (permissionState === 'granted') {
                setHasPermission(true);
                await startCamera();
            } else {
                setErrorMessage("Sensor permissions required.");
            }
        } catch (error) {
            setHasPermission(true);
            await startCamera();
        }
    };

    // --- 4. Sensor Logic ---
    const sensorListener = useCallback((event: DeviceMotionEvent) => {
        const { acceleration, rotationRate } = event;
        sensorDataBuffer.current.push({
            timestamp: Date.now(),
            accel: acceleration ? { x: acceleration.x || 0, y: acceleration.y || 0, z: acceleration.z || 0 } : null,
            gyro: rotationRate ? { alpha: rotationRate.alpha || 0, beta: rotationRate.beta || 0, gamma: rotationRate.gamma || 0 } : null,
        });
        if (sensorDataBuffer.current.length > 500) {
            sensorDataBuffer.current.splice(0, sensorDataBuffer.current.length - 500);
        }
    }, []);

    useEffect(() => {
        if (hasPermission) {
            window.addEventListener('devicemotion', sensorListener);
            return () => window.removeEventListener('devicemotion', sensorListener);
        }
    }, [hasPermission, sensorListener]);

    const interpolateSensorData = (timestamp: number) => {
        const buffer = sensorDataBuffer.current;
        if (buffer.length < 2) return { accel: null, gyro: null };

        let before: SensorData | null = null;
        let after: SensorData | null = null;
        
        for (let i = buffer.length - 1; i >= 0; i--) {
            if (buffer[i].timestamp <= timestamp) {
                before = buffer[i];
                if (i + 1 < buffer.length) after = buffer[i + 1];
                break;
            }
        }

        if (!before || !after) return { accel: before?.accel || null, gyro: before?.gyro || null };

        const t = (timestamp - before.timestamp) / (after.timestamp - before.timestamp);

        const lerp = (v1: any, v2: any) => {
            if (!v1 || !v2) return null;
            const res: any = {};
            Object.keys(v1).forEach(key => {
                res[key] = v1[key] + (v2[key] - v1[key]) * t;
            });
            return res;
        };

        return {
            accel: lerp(before.accel, after.accel),
            gyro: lerp(before.gyro, after.gyro),
        };
    };

    // --- 5. Main Loop (FaceMesh + OpenCV + DRAWING) ---
    const onFaceMeshResults = useCallback((results: any) => {
        const canvasCtx = canvasRef.current?.getContext('2d', { willReadFrequently: true });
        if (!canvasCtx || !videoRef.current || !canvasRef.current) return;

        const { videoWidth, videoHeight } = videoRef.current;
        if (canvasRef.current.width !== videoWidth || canvasRef.current.height !== videoHeight) {
            canvasRef.current.width = videoWidth;
            canvasRef.current.height = videoHeight;
        }
        
        // --- 1. CLEAR CANVAS (‡∏•‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ó‡∏∏‡∏Å‡πÄ‡∏ü‡∏£‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏î‡πÉ‡∏´‡∏°‡πà) ---
        canvasCtx.clearRect(0, 0, videoWidth, videoHeight);
        
        let faceLandmarks: FaceMeshResult | null = null;
        let faceBoundingBox = null;

        // --- 2. DRAWING LOGIC (‡∏ß‡∏≤‡∏î‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤ ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏±‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏°) ---
        if (results.multiFaceLandmarks && results.multiFaceLandmarks[0]) {
            const landmarks = results.multiFaceLandmarks[0];
            
            // ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            faceLandmarks = {
                all: landmarks,
                specific: FACEMESH_LANDMARK_INDICES.map(i => landmarks[i]),
                flat: FACEMESH_LANDMARK_INDICES.flatMap(i => [landmarks[i].x, landmarks[i].y, landmarks[i].z])
            };

            // A. ‡∏ß‡∏≤‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ (468 ‡∏à‡∏∏‡∏î) - ‡∏™‡∏µ‡∏ü‡πâ‡∏≤‡∏à‡∏≤‡∏á‡πÜ
            canvasCtx.fillStyle = 'rgba(0, 255, 255, 0.4)'; // Cyan, ‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÅ‡∏™‡∏á
            landmarks.forEach((lm: Point) => {
                const x = lm.x * videoWidth;
                const y = lm.y * videoHeight;
                canvasCtx.beginPath();
                canvasCtx.arc(x, y, 1, 0, 2 * Math.PI); // ‡∏à‡∏∏‡∏î‡πÄ‡∏•‡πá‡∏Å
                canvasCtx.fill();
            });

            // B. ‡∏ß‡∏≤‡∏î‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (28 ‡∏à‡∏∏‡∏î) - ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏™‡∏ß‡πà‡∏≤‡∏á
            canvasCtx.fillStyle = '#00FF00'; // Green

            faceLandmarks.specific.forEach((lm: Point) => {
                const x = lm.x * videoWidth;
                const y = lm.y * videoHeight;
                canvasCtx.beginPath();
                canvasCtx.arc(x, y, 2, 0, 2 * Math.PI); // ‡∏à‡∏∏‡∏î‡πÉ‡∏´‡∏ç‡πà
                canvasCtx.fill();
                canvasCtx.stroke();
            });

            // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Bounding Box ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenCV
            const xs = landmarks.map((l: Point) => l.x);
            const ys = landmarks.map((l: Point) => l.y);
            faceBoundingBox = {
                xMin: Math.min(...xs), xMax: Math.max(...xs),
                yMin: Math.min(...ys), yMax: Math.max(...ys),
            };
        }

        // --- 3. OpenCV Processing & bg_variance Calculation ---
        const cv = (window as any).cv;
        let currentBgVariance = 0; 

        if (cv && cv.Mat && videoWidth > 0) {
            let currentFrame: any = null;
            let currentGray: any = null;
            let mask: any = null;
            let tempPoints: any = null;
            let nextPoints: any = null;
            let status: any = null;
            let err: any = null;

            try {
                // ‡∏™‡∏£‡πâ‡∏≤‡∏á Mat ‡∏à‡∏≤‡∏Å Video (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ß‡∏≤‡∏î‡∏ó‡∏±‡∏ö‡∏•‡∏á Canvas ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ß‡∏≤‡∏î‡∏à‡∏∏‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß)
                currentFrame = new cv.Mat(videoHeight, videoWidth, cv.CV_8UC4);
                
                // *Hack*: ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å Video element ‡∏°‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏≠‡∏¢‡πà‡∏≤‡πÑ‡∏õ drawImage ‡∏ó‡∏±‡∏ö‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ß‡∏≤‡∏î
                // ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏∑‡∏≠: ‡∏™‡∏£‡πâ‡∏≤‡∏á canvas ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ OffscreenCanvas ‡πÅ‡∏ï‡πà‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠
                // ‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å Canvas ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏≤‡∏î‡∏à‡∏∏‡∏î (‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏ß‡∏≤‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß)
                // ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ drawImage ‡∏à‡∏≤‡∏Å video ‡∏•‡∏á‡∏ö‡∏ô Mat ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏ú‡πà‡∏≤‡∏ô temporary canvas ‡∏´‡∏£‡∏∑‡∏≠
                // ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢: ‡πÉ‡∏´‡πâ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ß‡πà‡∏≤ OpenCV ‡∏à‡∏∞ process ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å videoRef)
                
                // ‡πÉ‡∏ä‡πâ canvasCtx ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á pixel data (‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏Å‡∏¥‡∏ô resource ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢)
                // ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å videoRef ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô canvas
                // ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏î video ‡∏•‡∏á canvas ‡∏´‡∏•‡∏±‡∏Å 
                // ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ offscreen logic ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ. ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å video ‡∏•‡∏á currentFrame ‡∏ï‡∏£‡∏á‡πÜ
                
                // ‡∏™‡∏£‡πâ‡∏≤‡∏á Canvas ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÉ‡∏ô Memory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å Video (‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å)
                const tempCanvas = document.createElement('canvas');
                tempCanvas.width = videoWidth;
                tempCanvas.height = videoHeight;
                const tempCtx = tempCanvas.getContext('2d');
                if(tempCtx) {
                    tempCtx.drawImage(videoRef.current, 0, 0, videoWidth, videoHeight);
                    const frameImageData = tempCtx.getImageData(0, 0, videoWidth, videoHeight);
                    currentFrame.data.set(frameImageData.data);
                }

                currentGray = new cv.Mat();
                cv.cvtColor(currentFrame, currentGray, cv.COLOR_RGBA2GRAY);

                if (prevGray.current) {
                    // 1. Initial Points Detection
                    if (!backgroundPoints.current || backgroundPoints.current.rows === 0) {
                        mask = new cv.Mat.zeros(videoHeight, videoWidth, cv.CV_8U);
                        if (faceBoundingBox) {
                            const x = faceBoundingBox.xMin * videoWidth - 20;
                            const y = faceBoundingBox.yMin * videoHeight - 20;
                            const w = (faceBoundingBox.xMax - faceBoundingBox.xMin) * videoWidth + 40;
                            const h = (faceBoundingBox.yMax - faceBoundingBox.yMin) * videoHeight + 40;
                            
                            if (x >= 0 && y >= 0 && x + w <= videoWidth && y + h <= videoHeight) {
                                cv.rectangle(mask, new cv.Point(x, y), new cv.Point(x + w, y + h), new cv.Scalar(255), -1);
                                cv.bitwise_not(mask, mask);
                            } else {
                                mask.setTo(new cv.Scalar(255));
                            }
                        } else {
                            mask.setTo(new cv.Scalar(255));
                        }
                        tempPoints = new cv.Mat();
                        cv.goodFeaturesToTrack(currentGray, tempPoints, 10, 0.1, 10, mask, 7, false, 0.04);
                        backgroundPoints.current = tempPoints; 
                    }

                    // 2. Optical Flow
                    if (backgroundPoints.current && backgroundPoints.current.rows > 0) {
                        nextPoints = new cv.Mat();
                        status = new cv.Mat();
                        err = new cv.Mat();
                        
                        cv.calcOpticalFlowPyrLK(prevGray.current, currentGray, backgroundPoints.current, nextPoints, status, err);
                        
                        const p0 = backgroundPoints.current.data32F;
                        const p1 = nextPoints.data32F;
                        const st = status.data;

                        let goodNewPoints = [];
                        let movements: number[] = [];

                        for (let i = 0; i < st.length; i++) {
                            if (st[i] === 1) {
                                goodNewPoints.push(p1[i * 2], p1[i * 2 + 1]);
                                const xOld = p0[i * 2];
                                const yOld = p0[i * 2 + 1];
                                const xNew = p1[i * 2];
                                const yNew = p1[i * 2 + 1];
                                const dist = Math.sqrt(Math.pow(xNew - xOld, 2) + Math.pow(yNew - yOld, 2));
                                movements.push(dist);
                            }
                        }

                        if (movements.length > 0) {
                            const mean = movements.reduce((a, b) => a + b, 0) / movements.length;
                            const sqDiffs = movements.map(val => Math.pow(val - mean, 2));
                            currentBgVariance = sqDiffs.reduce((a, b) => a + b, 0) / movements.length;
                        }

                        if (backgroundPoints.current) backgroundPoints.current.delete();
                        backgroundPoints.current = goodNewPoints.length > 0 
                             ? cv.matFromArray(goodNewPoints.length / 2, 1, cv.CV_32FC2, goodNewPoints)
                             : null;
                    }
                }

                if (prevGray.current) prevGray.current.delete();
                prevGray.current = currentGray;
                currentGray = null;

            } catch (e) {
                console.warn("OpenCV Error:", e);
                if (backgroundPoints.current) { backgroundPoints.current.delete(); backgroundPoints.current = null; }
                if (prevGray.current) { prevGray.current.delete(); prevGray.current = null; }
            } finally {
                if (currentFrame) currentFrame.delete();
                if (currentGray) currentGray.delete();
                if (mask) mask.delete();
                if (nextPoints) nextPoints.delete();
                if (status) status.delete();
                if (err) err.delete();
            }
        }

        // --- 4. Recording Logic ---
        if (isRecordingRef.current) { 
            const timestamp = Date.now();
            const { accel, gyro } = interpolateSensorData(timestamp);
            const opticalFlowPoints = backgroundPoints.current ? Array.from(backgroundPoints.current.data32F as number[]) : [];
            
            // --- [‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û (Simple Capture) ---
            let imageBase64 = null;
            
            // ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ readyState ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà‡∏°‡∏µ video ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏Å‡πá‡∏û‡∏≠
            if (videoRef.current && videoRef.current.videoWidth > 0) {
                try {
                    const videoEl = videoRef.current;
                    const tempCanvas = document.createElement('canvas');
                    
                    // ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏•‡∏á (480px) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ó‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    const scale = 480 / videoEl.videoWidth;
                    tempCanvas.width = 480;
                    tempCanvas.height = videoEl.videoHeight * scale;
                    
                    const tempCtx = tempCanvas.getContext('2d');
                    if (tempCtx) {
                        tempCtx.drawImage(videoEl, 0, 0, tempCanvas.width, tempCanvas.height);
                        // ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Base64 (.jpg)
                        imageBase64 = tempCanvas.toDataURL('image/jpeg', 0.7);
                    }
                } catch (err) {
                    console.error("‚ùå Capture Error:", err);
                }
            } else {
                console.warn("‚ö†Ô∏è Video not ready for capture");
            }
            // ----------------------------------------------------

            recordedData.current.push({
                timestamp,
                faceMesh: faceLandmarks ? faceLandmarks.flat : null,
                sensors: { accel, gyro },
                opticalFlow: opticalFlowPoints,
                bg_variance: currentBgVariance,
                image: imageBase64 // <--- ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ
            });
            
            // --- Log ‡πÄ‡∏ä‡πá‡∏Ñ‡∏´‡∏ô‡πâ‡∏≤‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ---
            if (recordedData.current.length % 30 === 0) {
                // ‡∏ñ‡πâ‡∏≤ imageBase64 ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "üì∏ Got Image"
                // ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á "‚ùå No Image"
                const hasImg = imageBase64 ? "üì∏ Got Image" : "‚ùå No Image";
                console.log(`Rec: ${recordedData.current.length} frames | ${hasImg}`);
            }
        }
    }, []); 

    const gameLoop = useCallback(async () => {
        if (!faceMeshRef.current || !videoRef.current || videoRef.current.readyState < 3) {
            animationFrameId.current = requestAnimationFrame(gameLoop);
            return;
        }
        await faceMeshRef.current.send({ image: videoRef.current });
        animationFrameId.current = requestAnimationFrame(gameLoop);
    }, []);

    useEffect(() => {
        if (hasPermission && libsLoaded) {
            startCamera().then(() => {
                animationFrameId.current = requestAnimationFrame(gameLoop);
            });
        }
        return () => {
            if (animationFrameId.current) cancelAnimationFrame(animationFrameId.current);
        };
    }, [hasPermission, libsLoaded, startCamera, gameLoop]);

    const toggleRecording = () => {
        if (isRecording) {
            // STOP RECORDING
            setIsRecording(false);
            isRecordingRef.current = false;
            console.log("Stopped. Total Frames:", recordedData.current.length);
            
            if (recordedData.current.length > 0) {
                // ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ upload ‡πÄ‡∏•‡∏¢ -> ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏´‡∏°‡∏î Review
                setIsReviewing(true);
            } else {
                setErrorMessage("No data collected.");
                setTimeout(() => setErrorMessage(null), 3000);
            }
        } else {
            // START RECORDING
            recordedData.current = [];
            setIsRecording(true);
            isRecordingRef.current = true;
            setUploadStatus('idle');
            console.log("Started Recording...");
        }
    };

    const handleConfirmUpload = () => {
        setIsReviewing(false); // ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Review
        uploadData({ scenario, data: recordedData.current }); // ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    };

    const handleDiscard = () => {
        setIsReviewing(false); // ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Review
        recordedData.current = []; // ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏¥‡πâ‡∏á
        console.log("Data discarded.");
    };

    const uploadData = async (payload: any) => {
        setUploadStatus('uploading');
        try {
            // ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô URL ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Ngrok
            const apiUrl = import.meta.env.VITE_API_URL || 'http://localhost:5000'; 
            
            const res = await fetch(`${apiUrl}/upload`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload),
            });
            if (!res.ok) throw new Error("Upload failed");
            setUploadStatus('success');
            console.log("Success!");
            setTimeout(() => setUploadStatus('idle'), 3000);
        } catch (err: any) {
            setUploadStatus('error');
            setErrorMessage(err.message);
            console.error("Upload Error:", err);
        }
    };

    if (!libsLoaded) {
        return (
            <div className="w-screen h-screen flex flex-col items-center justify-center bg-gray-900 text-white">
                <LoadingSpinner />
                <p className="mt-4 text-lg">Initializing...</p>
            </div>
        );
    }

    return (
        <div className="relative w-screen h-screen overflow-hidden bg-black">
            <video ref={videoRef} autoPlay playsInline muted className="absolute inset-0 w-full h-full object-contain transform -scale-x-100" />
            <canvas ref={canvasRef} className="absolute inset-0 w-full h-full object-contain transform -scale-x-100" />
            
            {!hasPermission && (
                <div className="absolute inset-0 bg-black/80 flex items-center justify-center z-50">
                    <button onClick={handleConnect} className="bg-blue-600 px-8 py-4 rounded-xl font-bold text-white shadow-2xl">
                        Start Camera & Sensors
                    </button>
                </div>
            )}

            {hasPermission && (
                <>
                    {/* --- [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Review Mode (Overlay) --- */}
                    {isReviewing && (
                        <div className="absolute inset-0 bg-black/80 z-50 flex flex-col items-center justify-center space-y-6">
                            <div className="text-white text-2xl font-bold">Recording Finished</div>
                            <div className="text-gray-300">
                                Captured Frames: <span className="text-yellow-400 font-mono text-xl">{recordedData.current.length}</span>
                            </div>
                            
                            <div className="flex gap-4 mt-4">
                                {/* ‡∏õ‡∏∏‡πà‡∏°‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á */}
                                <button 
                                    onClick={handleDiscard}
                                    className="px-8 py-4 bg-gray-600 hover:bg-gray-700 text-white rounded-xl font-bold text-lg"
                                >
                                    ‚ùå Discard & Retake
                                </button>
                                
                                {/* ‡∏õ‡∏∏‡πà‡∏°‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô Save */}
                                <button 
                                    onClick={handleConfirmUpload}
                                    className="px-8 py-4 bg-green-600 hover:bg-green-700 text-white rounded-xl font-bold text-lg shadow-lg border-2 border-green-400"
                                >
                                    ‚úÖ Confirm Save
                                </button>
                            </div>
                        </div>
                    )}
                    {/* ------------------------------------------- */}

                    {/* UI ‡πÄ‡∏î‡∏¥‡∏° (‡∏ã‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô Review ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏î‡∏ã‡πâ‡∏≥) */}
                    {!isReviewing && (
                        <div className="absolute bottom-0 w-full p-6 bg-black/60 backdrop-blur-md flex items-center gap-4 z-40">
                            <select 
                                value={scenario} 
                                onChange={(e) => setScenario(e.target.value as Scenario)}
                                disabled={isRecording}
                                className="bg-gray-800 text-white p-3 rounded-lg flex-1"
                            >
                                <option value="REAL_Normal">Real - Normal</option>
                                <option value="REAL_WhiteWall">Real - White Wall</option>
                                <option value="REAL_Backlight">Real - Backlight</option>
                                <option value="REAL_Walking">Real - Walking</option>
                                <option value="Spoof_2DWall">Spoof - Photo Wall</option>
                                <option value="Spoof_2DScreen">Spoof - Photo Screen</option>
                                <option value="Spoof_VideoReplay">Spoof - Video Replay</option>
                            </select>

                            <button onClick={toggleRecording} className={`p-5 rounded-full ${isRecording ? 'bg-red-500' : 'bg-green-500'}`}>
                                {isRecording ? <StopIcon /> : <RecordIcon />}
                            </button>

                            <div className="flex-1 flex justify-end">
                                <Toast status={uploadStatus} message={errorMessage} />
                            </div>
                        </div>
                    )}
                </>
            )}
        </div>
    );
};

const Toast: React.FC<{ status: UploadStatus; message: string | null }> = ({ status, message }) => {
    if (status === 'idle') return null;
    const config = {
        uploading: { icon: <LoadingSpinner />, text: "Uploading...", color: "bg-blue-600" },
        success: { icon: <CheckCircleIcon />, text: "Done!", color: "bg-green-600" },
        error: { icon: <ExclamationTriangleIcon />, text: message || "Error", color: "bg-red-600" }
    };
    const { icon, text, color } = config[status];
    return (
        <div className={`flex items-center gap-2 px-4 py-2 rounded-lg text-white ${color}`}>
            {icon} <span>{text}</span>
        </div>
    );
};

export default App;